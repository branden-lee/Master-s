---
title: "Assignment 2"
author: "Branden Lee 20877653"
date: "21/03/2021"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

### Question 6

#### a)
```{r}
library(ggplot2)

set.seed(0)

posterior <- function(x) dcauchy(x, location=1.3, scale=1) * dcauchy(x, location=15, scale=1)
```
$$\begin{aligned} \pi(\theta\mid x_1=1.3, x_2=15) &\propto \pi(\theta) f(x_1,x_2\mid \theta)\\ &= \left[\frac{1}{\pi((1.3-\theta)^2+1)}\right] \left[\frac{1}{\pi((15-\theta)^2+1)}\right] \end{aligned}$$
```{r, echo=FALSE}
ggplot(data.frame(x=c(-12, 30)), aes(x)) + stat_function(fun=function(x) posterior(x), n=401) +
  xlab(expression(theta)) + ylab(expression(pi(theta|x[1],x[2])))
```


```{r}
## Part b
generate_sample <- function(init, sample_size, scale) {
  theta <- init
  sample <- c()
  n <- 1
  num_accepted <- 0
  while (length(sample) < sample_size) {
    proposal <- rcauchy(1, theta, scale)
    r <- posterior(proposal) / posterior(theta)
    if (r > 1 || runif(1) <= r) {
      theta <- proposal
      }
    if (n > 5000) {sample <- c(sample, theta)}
    n <- n + 1
  }
  return(list(sample, num_accepted / (5000 + sample_size)))
}
```


```{r}
## Part c
# Proposal distribution for temperatures 
temp_density <- function(i) {
  prob <- rep(0, 10)
  if (i == 1) {prob[2] = 1}
  else if (i == 10) {prob[9] = 1}
  else {
      prob[i-1] = .5
      prob[i+1] = .5
  }
  return(prob)
}

# Inverse temperatures in descending order
inv_t <- c(seq(1, 0.1, by=-0.1))

# Distribution of temperatures, used as tuning parameter
p <- 4 ^ seq(10, 1, by=-1)

# Generating x given temperature
temperature_sample <- function(i, sd) {
  theta <- 8.15 # Median chosen as initial point
  
  for (n in seq(2000)) {
    proposal <- rnorm(1, theta, sd)
    r <- (posterior(proposal) / posterior(theta))^inv_t[i]
    if (r > 1 || runif(1) <= r) {theta <- proposal}
  }
  return(theta)
}

simulated_tempering <- function(init, sample_size, s) {
  sample <- c()
  n <- 1
  i <- init
  
  num_accepted <- 0
  while (length(sample) < sample_size) {
    proposal_density <- temp_density(i)

    x <- temperature_sample(i, s)
    i_new <- sample(10, size=1, prob=proposal_density)

    # r <- inv_t[i_new] * log(posterior(x)) + log(proposal_density[i_new]) -
    #   inv_t[i] * log(posterior(x)) - log(temp_density(i_new)[i])
    
    r <- posterior(x)^inv_t[i_new] * proposal_density[i_new] * p[i_new] /
     (posterior(x)^inv_t[i] * temp_density(i_new)[i] * p[i])
    
    if (r > 1 || runif(1) <= r) {
      i <- i_new
      num_accepted <- num_accepted + 1
      }
    if (n > 5000) {sample <- c(sample, x)}
    n <- n + 1
  }
  return(list(sample, num_accepted / (5000+sample_size)))
}
```

#### d)
```{r}
outputb <- generate_sample(3, 5000, 1)
sampleb <- outputb[[1]]
acceptance_rateb <- outputb[[2]]

outputc <- simulated_tempering(5, 5000, .2)
samplec <- outputc[[1]]
acceptance_ratec <- outputc[[2]]
```
For the Metropolis algorithm implemented in part b, the first 5000 observations were discarded and only the following 5000 were included in the plot. Setting the scale parameter $\gamma$ of the Cauchy proposal distribution to 1 resulted in the data plotted in the first histogram below: of all values tried $\gamma=1$ resulted in a good trade off between a relatively high acceptance rate compared to other values (`r round(acceptance_rateb, 5)`) and a good approximation to the target density. 
For the simulated tempering algorithm implemented in part c, the temperature ladder $T_i = 1 - 0.1 * i$ for  $i=0,..., 9$ was used, with marginal probabilities $p(i)$ of temperatures were tuned for faster mixing: various values were tried, with $p(i) = \frac{3^{10-i}}{c}$ ultimately resulting in the best tradeoff between a reasonably high acceptance rate (`r round(acceptance_ratec, 5)`) and accuracy of the approximation of the target posterior density, among all values tried. To obtain $X^{(i+1)}\mid i$, a random walk Metropolis-Hastings with a Gaussian proposal was used: the first 2000 observations were discarded before selecting the sample, and the standard deviation of the Gaussian was also used as a tuning parameter: multiple values were tried, with a standard deviation of .2 being the final choice as among all other values tried, it resulted in the best approximation. To generate the final sample, the first 5000 observations were discarded and the remaining 5000 resulted in the data as plotted below.  
Both result in reasonable approximations to the posterior, however it seems like the Metropolis algorithm in part b results in a good sample while being quicker.

```{r echo=FALSE}
dfb <- data.frame(x=sampleb)
plot2 <- ggplot(dfb, aes(x=x, y=..density..)) + geom_histogram(binwidth=.5) +
  xlab(expression(theta)) + labs(title='Empirical Density '~theta~ ', Metropolis')

  
dfc <- data.frame(x=samplec)
plot3 <- ggplot(dfc, aes(x=x, y=..density..)) + geom_histogram(binwidth=.5) +
  xlab(expression(theta)) + labs(title=bquote('Empirical Density '~theta~ ', Simulated Tempering'))

plot2
plot3
```