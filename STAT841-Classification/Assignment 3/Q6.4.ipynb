{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.porter import *\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tabulate import tabulate\n",
    "import statsmodels.api as sm\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "training_indices = np.random.choice(1000, size=800, replace=False)\n",
    "\n",
    "# Read and process text file, by removing punctuation, normalizing to lower case letters and applying a porter stemmer.\n",
    "# Return a dataframe with 3 columns: one for the Document stored as a list of words, another for the word count and the last\n",
    "# for the positive/negative sentiment.\n",
    "def process_text(stem, stop):\n",
    "    temp_data =[]\n",
    "    \n",
    "    translations = dict((ord(char), \" \") for char in string.punctuation)\n",
    "        \n",
    "    f=open('amazon_cells_labelled.txt')\n",
    "    for line in f:\n",
    "        document = line.strip().split('\\t')   \n",
    "        tokens = word_tokenize(document[0].translate(translations).lower())\n",
    "        word_count = len(tokens)\n",
    "        if stop is True:\n",
    "            stop_words = set(stopwords.words('english')) \n",
    "            if stem is True:\n",
    "                stemmer = PorterStemmer()\n",
    "                tokens = [stemmer.stem(w) for w in tokens if w not in stop_words]\n",
    "            else:\n",
    "                tokens = [w for w in tokens if w not in stop_words]\n",
    "        else:\n",
    "            if stem is True:\n",
    "                stemmer = PorterStemmer()\n",
    "                tokens = [stemmer.stem(w) for w in tokens]\n",
    "            else:\n",
    "                tokens = [w for w in tokens]\n",
    "        temp_data.append({'Document': tokens, 'word_count': word_count, 'y': int(document[1])})\n",
    "        \n",
    "    return(pd.DataFrame(temp_data, columns=['Document', 'word_count', 'y']))\n",
    "\n",
    "# Create a matrix consisting of non-negative integers from a dataframe data which is meant to be created from process_text.\n",
    "# A 1 in entry in row i and column j corresponds to document i having the jth word.\n",
    "def token_matrix(data):\n",
    "    new_data = pd.DataFrame(index=data.index)\n",
    "    for n in range(len(data.index)):\n",
    "        for word in data.iloc[n]['Document']:\n",
    "            if word not in new_data.columns:\n",
    "                new_data[word] = 0\n",
    "            new_data[word].iloc[n] += 1\n",
    "    return new_data\n",
    "\n",
    "# Generate a matrix of n_grams from a matrix created from token_matrix with specified threshold.\n",
    "def generate_n_grams(token_matrix, threshold):\n",
    "    col = token_matrix.columns[(token_matrix.sum(axis=0) >= threshold) == True]\n",
    "    return token_matrix[col]\n",
    "\n",
    "# Fit a random forest classifier on the data and calculate prediction accuracy on test set.\n",
    "def accuracy(training, test):\n",
    "    model = RandomForestClassifier().fit(X=training[training.columns.difference(['Document', 'y'])], y=training['y'])\n",
    "    return accuracy_score(test['y'], model.predict(X=test[test.columns.difference(['Document', 'y'])]))\n",
    "    \n",
    "### part a\n",
    "df = process_text(True, True)\n",
    "df2 = token_matrix(df)\n",
    "\n",
    "n_grams = generate_n_grams(df2, 5)\n",
    "n_grams_ind = n_grams.ge(0.5).astype(int)\n",
    "df_a = pd.concat([df, n_grams], axis=1)\n",
    "df_ind = pd.concat([df, n_grams_ind], axis=1)\n",
    "\n",
    "accuracy_count = accuracy(df_a.iloc[training_indices], df_a.iloc[~training_indices])\n",
    "accuracy_ind = accuracy(df_ind.iloc[training_indices], df_ind.iloc[~training_indices])\n",
    "\n",
    "print(tabulate(\n",
    "    {\"N-gram Type\": ['Count', 'Indicator'], \n",
    "     \"Accuracy\": [accuracy_count, accuracy_ind]},\n",
    "     headers=\"keys\"), file=open('Q 6.4 a output.txt', 'w'))\n",
    "\n",
    "### part b\n",
    "n_grams_3 = generate_n_grams(df2, 3)\n",
    "df_3 = pd.concat([df, n_grams_3], axis=1)\n",
    "\n",
    "n_grams_7 = generate_n_grams(df2, 7)\n",
    "df_7 = pd.concat([df, n_grams_7], axis=1)\n",
    "\n",
    "n_grams_9 = generate_n_grams(df2, 9)\n",
    "df_9 = pd.concat([df, n_grams_9], axis=1)\n",
    "\n",
    "print(tabulate({\"Threshold\": [3, 5, 7, 9], \n",
    "     \"Accuracy\":\n",
    "     [accuracy(df_3.iloc[training_indices], df_3.iloc[~training_indices]),\n",
    "      accuracy_count,\n",
    "      accuracy(df_7.iloc[training_indices], df_7.iloc[~training_indices]),\n",
    "      accuracy(df_9.iloc[training_indices], df_9.iloc[~training_indices])\n",
    "     ]},\n",
    "     headers=\"keys\"), file=open('Q 6.4 b output.txt', 'w'))\n",
    "\n",
    "### part c\n",
    "stem_no_stop = process_text(True, False)\n",
    "n_grams_stem_no_stop = generate_n_grams(token_matrix(stem_no_stop), 5)\n",
    "df_stem_no_stop = pd.concat([stem_no_stop, n_grams_stem_no_stop], axis=1)\n",
    "\n",
    "no_stem_stop = process_text(False, True)\n",
    "n_grams_no_stem_stop = generate_n_grams(token_matrix(no_stem_stop), 5)\n",
    "df_no_stem_stop = pd.concat([no_stem_stop, n_grams_no_stem_stop], axis=1)\n",
    "\n",
    "no_stem_no_stop = process_text(False, False)\n",
    "n_grams_no_stem_no_stop = generate_n_grams(token_matrix(no_stem_no_stop), 5)\n",
    "df_no_stem_no_stop = pd.concat([no_stem_no_stop, n_grams_no_stem_no_stop], axis=1)\n",
    "\n",
    "print(tabulate(\n",
    "    {\"With Stemming\": \n",
    "     [accuracy_count, \n",
    "      accuracy(df_stem_no_stop.iloc[training_indices], df_stem_no_stop.iloc[~training_indices])], \n",
    "     \"Without Stemming\":\n",
    "     [accuracy(df_no_stem_stop.iloc[training_indices], df_no_stem_stop.iloc[~training_indices]), \n",
    "      accuracy(df_no_stem_no_stop.iloc[training_indices], df_no_stem_no_stop.iloc[~training_indices])]},\n",
    "     headers=\"keys\", showindex=['Remove Stopwords', 'Keep Stopwords']), file=open('Q 6.4 c1 output.txt', 'w'))\n",
    "\n",
    "print(tabulate(\n",
    "    {\"With Stemming\": [len(n_grams.columns), len(n_grams_stem_no_stop.columns)], \n",
    "     \"Without Stemming\": [len(n_grams_no_stem_stop.columns), len(n_grams_no_stem_no_stop.columns)]},\n",
    "     headers=\"keys\", showindex=['Remove Stopwords', 'Keep Stopwords']), file=open('Q 6.4 c2 output.txt', 'w'))\n",
    "\n",
    "### part d\n",
    "# Try to fit logistic regression to the data, and if it fails print output describing the type of error that occurred.\n",
    "try:\n",
    "    logistic_model = sm.Logit(endog=df_a['y'].iloc[training_indices], \n",
    "                      exog=df_a[df_a.columns.difference(['Document', 'y'])].iloc[training_indices])\n",
    "    logistic_results = logistic_model.fit()\n",
    "except Exception as e:\n",
    "    print(e, file=open('Q 6.4 d output.txt', 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
